<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Supervised learning</title>
    <meta charset="utf-8" />
    <meta name="author" content=" Data Analytics for Psychology and Business" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="baselrbootcamp.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Supervised learning
### <a href='https://cdsbasel.github.io/dataanalytics/'> Data Analytics for Psychology and Business </a> <br> <a href='https://cdsbasel.github.io/dataanalytics/menu/materials.html'> <i class='fas fa-clock' style='font-size:.9em;'></i> </a>  <a href='https://cdsbasel.github.io/dataanalytics/'> <i class='fas fa-home' style='font-size:.9em;' ></i> </a>  <a href='mailto:rui.mata@unibas.ch'> <i class='fas fa-envelope' style='font-size: .9em;'></i>
### April 2019

---


layout: true

&lt;div class="my-footer"&gt;
  &lt;span style="text-align:center"&gt;
    &lt;span&gt; 
      &lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/&gt;
    &lt;/span&gt;
    &lt;a href="https://cdsbasel.github.io/dataanalytics/"&gt;
      &lt;span style="padding-left:82px"&gt; 
        &lt;font color="#7E7E7E"&gt;
          cdsbasel.github.io/dataanalytics/
        &lt;/font&gt;
      &lt;/span&gt;
    &lt;/a&gt;
    &lt;a href="https://cdsbasel.github.io/dataanalytics/"&gt;
      &lt;font color="#7E7E7E"&gt;
       Data Analytics for Psychology and Business | February 2019
      &lt;/font&gt;
    &lt;/a&gt;
    &lt;/span&gt;
  &lt;/div&gt; 

---









class: center, middle

&lt;a&gt;&lt;h1&gt;Fitting&lt;/h1&gt;&lt;/a&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Evaluation&lt;/h1&gt;&lt;/font&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Tuning&lt;/h1&gt;&lt;/font&gt;

---

.pull-left45[

# Fitting

&lt;p style="padding-top:1px"&gt;&lt;/p&gt;

Models are actually &lt;high&gt;families of models&lt;/high&gt;, with every parameter combination specifying a different model. 

To fit a model means to &lt;high&gt;identify&lt;/high&gt; from the family of models &lt;high&gt;the specific model that fits the data best&lt;/high&gt;. 

]

.pull-right45[

&lt;br&gt;&lt;br&gt;

&lt;p align = "center"&gt;
&lt;img src="image/curvefits.png" height=480px&gt;&lt;br&gt;
&lt;font style="font-size:10px"&gt;adapted from &lt;a href="https://www.explainxkcd.com/wiki/index.php/2048:_Curve-Fitting"&gt;explainxkcd.com&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;

]

---

# Which of these models is better? Why?

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---

# Which of these models is better? Why?

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---

# Loss function

.pull-left45[

Possible &lt;high&gt;the most important concept&lt;/high&gt; in statistics and machine learning.

The loss function defines some &lt;high&gt;summary of the errors committed by the model&lt;/high&gt;.

&lt;p style="padding-top:7px"&gt;

`$$\Large Loss = f(Error)$$`

&lt;p style="padding-top:7px"&gt;

&lt;u&gt;Two purposes&lt;/u&gt;

&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
&lt;tr&gt;
  &lt;td&gt;
    &lt;b&gt;Purpose&lt;/b&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;b&gt;Description&lt;/b&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    Fitting
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    Find parameters that minimize loss function.
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
    Evaluation
  &lt;/td&gt;
  &lt;td&gt;
    Calculate loss function for fitted model.
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

]


.pull-right45[

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-4-1.png" width="90%" style="display: block; margin: auto;" /&gt;


]

---

class: center, middle

&lt;high&gt;&lt;h1&gt;Regression&lt;/h1&gt;&lt;/high&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Decision Trees&lt;/h1&gt;&lt;/font&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Random Forests&lt;/h1&gt;&lt;/font&gt;



---

# Regression

.pull-left45[

In [regression](https://en.wikipedia.org/wiki/Regression_analysis), the criterion `\(Y\)` is modeled as the &lt;high&gt;sum&lt;/high&gt; of &lt;high&gt;features&lt;/high&gt; `\(X_1, X_2, ...\)` &lt;high&gt;times weights&lt;/high&gt; `\(\beta_1, \beta_2, ...\)` plus `\(\beta_0\)` the so-called the intercept.

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;


`$$\large \hat{Y} =  \beta_{0} + \beta_{1} \times X_1 + \beta_{2} \times X2 + ...$$`

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

The weight `\(\beta_{i}\)` indiciates the &lt;high&gt;amount of change&lt;/high&gt; in `\(\hat{Y}\)` for a change of 1 in `\(X_{i}\)`.

Ceteris paribus, the &lt;high&gt;more extreme&lt;/high&gt; `\(\beta_{i}\)`, the &lt;high&gt;more important&lt;/high&gt; `\(X_{i}\)` for the prediction of `\(Y\)` &lt;font style="font-size:12px"&gt;(Note: the scale of `\(X_{i}\)` matters too!).&lt;/font&gt;

If `\(\beta_{i} = 0\)`, then `\(X_{i}\)` &lt;high&gt;does not help&lt;/high&gt; predicting `\(Y\)`



]

.pull-right45[

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-5-1.png" width="90%" style="display: block; margin: auto;" /&gt;


]


---

# Regression loss

.pull-left45[

&lt;p&gt;

&lt;font style="font-size:24"&gt;&lt;b&gt; Mean Squared Error (MSE)&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;br&gt;&lt;high&gt;Average squared distance&lt;/high&gt; between predictions and true values?&lt;br&gt;

$$ MSE = \frac{1}{n}\sum_{i \in 1,...,n}(Y_{i} - \hat{Y}_{i})^{2}$$

&lt;br&gt;&lt;font style="font-size:24"&gt;&lt;b&gt; Mean Absolute Error (MAE)&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;br&gt;&lt;high&gt;Average absolute distance&lt;/high&gt; between predictions and true values?&lt;br&gt;

$$ MAE = \frac{1}{n}\sum_{i \in 1,...,n} \lvert Y_{i} - \hat{Y}_{i} \rvert$$


&lt;/p&gt;

]

.pull-right45[

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;" /&gt;


]


---

# Fitting

.pull-left45[

There are two fundamentally different ways to find the set of parameters that minimizes loss.

&lt;font style="font-size:24"&gt;&lt;b&gt; Analytically &lt;/b&gt;

In rare cases, the parameters can be &lt;high&gt;directly calculated&lt;/high&gt;, e.g., using the &lt;i&gt;normal equation&lt;/i&gt;: 

`$$\boldsymbol \beta = (\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol y$$`

&lt;font style="font-size:24"&gt;&lt;b&gt; Numerically &lt;/b&gt;

In most cases, parameters need to be found using a &lt;high&gt;directed trial and error&lt;/high&gt;, e.g., &lt;i&gt;gradient descent&lt;/i&gt;: 

`$$\boldsymbol \beta_{n+1} = \boldsymbol \beta_{n}+\gamma \nabla F(\boldsymbol \beta_{n})$$`

]

.pull-right45[

&lt;p align = "center"&gt;
&lt;img src="image/gradient.png" height=420px&gt;&lt;br&gt;
&lt;font style="font-size:10px"&gt;adapted from &lt;a href="https://me.me/i/machine-learning-gradient-descent-machine-learning-machine-learning-behind-the-ea8fe9fc64054eda89232d7ffc9ba60e"&gt;me.me&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;

]


---

.pull-left45[

# Fitting

&lt;p style="padding-top:1px"&gt;&lt;/p&gt;

There are two fundamentally different ways to find the set of parameters that minimizes loss.

&lt;font style="font-size:24"&gt;&lt;b&gt; Analytically &lt;/b&gt;

In rare cases, the parameters can be &lt;high&gt;directly calculated&lt;/high&gt;, e.g., using the &lt;i&gt;normal equation&lt;/i&gt;: 

`$$\boldsymbol \beta = (\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol y$$`

&lt;font style="font-size:24"&gt;&lt;b&gt; Numerically &lt;/b&gt;

In most cases, parameters need to be found using a &lt;high&gt;directed trial and error&lt;/high&gt;, e.g., &lt;i&gt;gradient descent&lt;/i&gt;: 

`$$\boldsymbol \beta_{n+1} = \boldsymbol \beta_{n}+\gamma \nabla F(\boldsymbol \beta_{n})$$`


]

.pull-right45[

&lt;br&gt;&lt;br2&gt;

&lt;p align = "center"&gt;
&lt;img src="image/gradient1.gif" height=250px&gt;&lt;br&gt;
&lt;font style="font-size:10px"&gt;adapted from &lt;a href="https://dunglai.github.io/2017/12/21/gradient-descent/
"&gt;dunglai.github.io&lt;/a&gt;&lt;/font&gt;&lt;br&gt;
&lt;img src="image/gradient2.gif" height=250px&gt;&lt;br&gt;
&lt;font style="font-size:10px"&gt;adapted from &lt;a href="https://dunglai.github.io/2017/12/21/gradient-descent/
"&gt;dunglai.github.io&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;

]

---

# 2 types of supervised problems

.pull-left5[

There are two types of supervised learning problems that can often be approached using the same model.

&lt;font style="font-size:24px"&gt;&lt;b&gt;Regression&lt;/b&gt;&lt;/font&gt;

Regression problems involve the &lt;high&gt;prediction of a quantitative feature&lt;/high&gt;. 

E.g., predicting the cholesterol level as a function of age. 

&lt;font style="font-size:24px"&gt;&lt;b&gt;Classification&lt;/b&gt;&lt;/font&gt;

Classification problems involve the &lt;high&gt;prediction of a categorical feature&lt;/high&gt;.   

E.g., predicting the type of chest pain as a function of age. 


]

.pull-right4[

&lt;p align = "center"&gt;
&lt;img src="image/twotypes.png" height=440px&gt;&lt;br&gt;
&lt;/p&gt;

]

---

# Logistic regression

.pull-left45[

In [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), the class criterion `\(Y \in (0,1)\)` is modeled also as the &lt;high&gt;sum of feature times weights&lt;/high&gt;, but with the prediction being transformed using a &lt;high&gt;logistic link function&lt;/high&gt;:

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

`$$\large \hat{Y} =  Logistic(\beta_{0} + \beta_{1} \times X_1 + ...)$$`

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

The logistic function &lt;high&gt;maps predictions to the range of 0 and 1&lt;/high&gt;, the two class values.

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

$$ Logistic(x) = \frac{1}{1+exp(-x)}$$

]

.pull-right45[

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-7-1.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---

# Logistic regression

.pull-left45[

In [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), the class criterion `\(Y \in (0,1)\)` is modeled also as the &lt;high&gt;sum of feature times weights&lt;/high&gt;, but with the prediction being transformed using a &lt;high&gt;logistic link function&lt;/high&gt;:

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

`$$\large \hat{Y} =  Logistic(\beta_{0} + \beta_{1} \times X_1 + ...)$$`

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

The logistic function &lt;high&gt;maps predictions to the range of 0 and 1&lt;/high&gt;, the two class values.

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

$$ Logistic(x) = \frac{1}{1+exp(-x)}$$

]

.pull-right45[

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-8-1.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---

# Classification loss - two ways

.pull-left45[

&lt;font style="font-size:24px"&gt;&lt;b&gt;Distance&lt;/b&gt;&lt;/font&gt;

Logloss is &lt;high&gt;used to fit the parameters&lt;/high&gt;, alternative distance measures are MSE and MAE.

`$$\small LogLoss = -\frac{1}{n}\sum_{i}^{n}(log(\hat{y})y+log(1-\hat{y})(1-y))$$`
`$$\small MSE = \frac{1}{n}\sum_{i}^{n}(y-\hat{y})^2, \: MAE = \frac{1}{n}\sum_{i}^{n} \lvert y-\hat{y} \rvert$$`

&lt;font style="font-size:24px"&gt;&lt;b&gt;Overlap&lt;/b&gt;&lt;/font&gt;

Does the &lt;high&gt;predicted class match the actual class&lt;/high&gt;. Often preferred for &lt;high&gt;ease of interpretation&lt;/high&gt;. 

`$$\small Loss_{01}=\frac{1}{n}\sum_i^n I(y \neq \lfloor \hat{y} \rceil)$$`

]

.pull-right45[

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-9-1.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---

# Confusion matrix

.pull-left45[

The confusion matrix &lt;high&gt;tabulates prediction matches and mismatches&lt;/high&gt; as a function of the true class.

The confusion matrix permits specification of a number of &lt;high&gt;helpful performance metrics&lt;/high&gt;.  

&lt;br&gt;

&lt;u&gt; Confusion matrix &lt;/u&gt;

&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
&lt;tr&gt;
  &lt;td&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;eq&gt;&lt;b&gt;y&amp;#770; = 1&lt;/b&gt;&lt;/eq&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;eq&gt;&lt;b&gt;y&amp;#770; = 0&lt;/b&gt;&lt;/eq&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;eq&gt;&lt;b&gt;y = 1&lt;/b&gt;&lt;/eq&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;font color="#6ABA9A"&gt; True positive (TP)&lt;/font&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;font color="#EA4B68"&gt; False negative (FN)&lt;/font&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
    &lt;eq&gt;&lt;b&gt;y = 0&lt;/b&gt;&lt;/eq&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;font color="#EA4B68"&gt; False positive (FP)&lt;/font&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;font color="#6ABA9A"&gt; True negative (TN)&lt;/font&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;


]

.pull-right45[


&lt;b&gt;Accuracy&lt;/b&gt;: Of all cases&lt;/i&gt;, what percent of predictions are correct?

`$$\small Acc. = \frac{TP + TN}{ TP + TN + FN + FP} = 1-Loss_{01}$$`

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

&lt;b&gt;Sensitivity&lt;/b&gt;: Of the truly Positive cases&lt;/i&gt;, what percent of predictions are correct?

`$$\small Sensitivity = \frac{TP}{ TP +FN }$$`

&lt;b&gt;Specificity&lt;/b&gt;: Of the truly Negative cases&lt;/i&gt;, what percent of predictions are correct?

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

`$$\small Specificity = \frac{TN}{ TN + FP }$$`

]


---

# Confusion matrix

.pull-left45[

The confusion matrix &lt;high&gt;tabulates prediction matches and mismatches&lt;/high&gt; as a function of the true class.

The confusion matrix permits specification of a number of &lt;high&gt;helpful performance metrics&lt;/high&gt;.  

&lt;br&gt;

&lt;u&gt; Confusion matrix &lt;/u&gt;

&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
&lt;tr&gt;
  &lt;td&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;eq&gt;&lt;b&gt;"Default"&lt;/b&gt;&lt;/eq&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;eq&gt;&lt;b&gt;"Repay"&lt;/b&gt;&lt;/eq&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;eq&gt;&lt;b&gt;Default&lt;/b&gt;&lt;/eq&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;font color="#6ABA9A"&gt; TP = 3&lt;/font&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;font color="#EA4B68"&gt; FN = 1&lt;/font&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
    &lt;eq&gt;&lt;b&gt;Repay&lt;/b&gt;&lt;/eq&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;font color="#EA4B68"&gt; FP = 1&lt;/font&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;font color="#6ABA9A"&gt; TN = 2&lt;/font&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;


]

.pull-right45[


&lt;b&gt;Accuracy&lt;/b&gt;: Of all cases&lt;/i&gt;, what percent of predictions are correct?

`$$\small Acc. = \frac{TP + TN}{ TP + TN + FN + FP} = 1-Loss_{01}$$`

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

&lt;b&gt;Sensitivity&lt;/b&gt;: Of the truly Positive cases&lt;/i&gt;, what percent of predictions are correct?

`$$\small Sensitivity = \frac{TP}{ TP +FN }$$`

&lt;b&gt;Specificity&lt;/b&gt;: Of the truly Negative cases&lt;/i&gt;, what percent of predictions are correct?

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

`$$\small Specificity = \frac{TN}{ TN + FP }$$`

]

---

class: center, middle

&lt;font color = "gray"&gt;&lt;h1&gt;Fitting&lt;/h1&gt;&lt;/font&gt;

&lt;a&gt;&lt;h1&gt;Evaluation&lt;/h1&gt;&lt;/a&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Tuning&lt;/h1&gt;&lt;/font&gt;


---

# Hold-out data

.pull-left45[

Model performance must be evaluated as true prediction on an &lt;high&gt;unseen data set&lt;/high&gt;.

The unseen data set can be &lt;high&gt;naturally&lt;/high&gt; occurring, e.g., using 2019 stock prizes to evaluate a model fit using 2018 stock prizes. 

More commonly unseen data is created by &lt;high&gt;splitting the available data&lt;/high&gt; into a training set and a test set. 

]


.pull-right45[

&lt;p align = "center"&gt;
&lt;img src="image/testdata.png" height=430px&gt;
&lt;/p&gt;

]

---

# Training

Training a model means to &lt;high&gt;fit the model&lt;/high&gt; to data by finding the parameter combination that &lt;high&gt;minizes some error function&lt;/high&gt;, e.g., mean squared error (MSE).


&lt;p align = "center" style="padding-top:30px"&gt;
&lt;img src="image/training_flow.png" height=350px&gt;
&lt;/p&gt;


---

# Test

To test a model means to &lt;high&gt;evaluate the prediction error&lt;/high&gt; for a fitted model, i.e., for a &lt;high&gt;fixed parameter combination&lt;/high&gt;.


&lt;p align = "center" style="padding-top:30px"&gt;
&lt;img src="image/testing_flow.png" height=350px&gt;
&lt;/p&gt;


---

.pull-left4[

&lt;br&gt;&lt;br&gt;
# Overfitting

Occurs when a model &lt;high&gt;fits data too closely&lt;/high&gt; and therefore &lt;high&gt;fails to reliably predict&lt;/high&gt; future observations. 

In other words, overfitting occurs when a model &lt;high&gt;'mistakes' random noise for a predictable signal&lt;/high&gt;.

More &lt;high&gt;complex models&lt;/high&gt; are more &lt;high&gt;prone to overfitting&lt;/high&gt;. 

]


.pull-right5[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/overfitting.png"&gt;
&lt;/p&gt;

]


---

# Overfitting

&lt;img src="SupervisedLearning_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;


---

class: center, middle

&lt;high&gt;&lt;h1&gt;Regression&lt;/h1&gt;&lt;/high&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Decision Trees&lt;/h1&gt;&lt;/font&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Random Forests&lt;/h1&gt;&lt;/font&gt;


---

# Regularized regression

.pull-left45[

Penalizes regression loss for having large `\(\beta\)` values using the &lt;high&gt;lambda &amp;lambda; tuning parameter&lt;/high&gt; and one of several penalty functions.

$$Regularized \;loss = \sum_i^n (y_i-\hat{y}_i)^2+\lambda \sum_j^p f(\beta_j)) $$
&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Name&lt;/b&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Function&lt;/b&gt;
  &lt;/td&gt; 
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Description&lt;/b&gt;
  &lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Lasso&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    |&amp;beta;&lt;sub&gt;j&lt;/sub&gt;|
  &lt;/td&gt; 
  &lt;td bgcolor="white"&gt;
    Penalize by the &lt;high&gt;absolute&lt;/high&gt; regression weights.
  &lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Ridge&lt;/i&gt;    
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &amp;beta;&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;
  &lt;/td&gt;  
  &lt;td bgcolor="white"&gt;
    Penalize by the &lt;high&gt;squared&lt;/high&gt; regression weights.
  &lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Elastic net&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    |&amp;beta;&lt;sub&gt;j&lt;/sub&gt;| + &amp;beta;&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;
  &lt;/td&gt; 
  &lt;td bgcolor="white"&gt;
    Penalize by Lasso and Ridge penalties.
  &lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;


]


.pull-right45[

&lt;p align = "center"&gt;
&lt;img src="image/bonsai.png"&gt;&lt;br&gt;
&lt;font style="font-size:10px"&gt;from &lt;a href="https://www.mallorcazeitung.es/leben/2018/05/02/bonsai-liebhaber-mallorca-kunst-lebenden/59437.html"&gt;mallorcazeitung.es&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;

]

---


.pull-left45[

# Regularized regression

&lt;p style="padding-top:1px"&gt;&lt;/p&gt;

Despite &lt;high&gt;superficial similarities&lt;/high&gt;, Lasso and Ridge show very different behavior.

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

&lt;b&gt;Ridge&lt;/b&gt;

By penalizing the most extreme &amp;beta;s most strongly, Ridge leads to (relatively) more &lt;high&gt;uniform &amp;beta;s&lt;/high&gt;.

&lt;p style="padding-top:10px"&gt;&lt;/p&gt;

&lt;b&gt;Lasso&lt;/b&gt;

By penalizing all &amp;beta;s equally, irrespective of magnitude, Lasso drives some &amp;beta;s to 0 resulting effectively in &lt;high&gt;automatic feature selection&lt;/high&gt;.

]

.pull-right45[

&lt;br&gt;

&lt;p align = "center"&gt;
&lt;font style="font-size:40"&gt;&lt;i&gt;Ridge&lt;/i&gt;&lt;/font&gt;&lt;br&gt;
  &lt;img src="image/ridge.png" height=210px&gt;&lt;br&gt;
  &lt;font style="font-size:10px"&gt;from &lt;a href="https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"&gt;James et al. (2013) ISLR&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;

&lt;p align = "center"&gt;
&lt;font style="font-size:40"&gt;&lt;i&gt;Lasso&lt;/i&gt;&lt;/font&gt;&lt;br&gt;
    &lt;img src="image/lasso.png" height=210px&gt;&lt;br&gt;
    &lt;font style="font-size:10px"&gt;from &lt;a href="https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"&gt;James et al. (2013) ISLR&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;

]


---

class: center, middle

&lt;font color = "gray"&gt;&lt;h1&gt;Regression&lt;/h1&gt;&lt;/font&gt;

&lt;high&gt;&lt;h1&gt;Decision Trees&lt;/h1&gt;&lt;/high&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Random Forests&lt;/h1&gt;&lt;/font&gt;

---

# CART

.pull-left45[

CART is short for &lt;high&gt;Classification and Regression Trees&lt;/high&gt;, which are often just called &lt;high&gt;Decision trees&lt;/high&gt;.

In [decision trees](https://en.wikipedia.org/wiki/Decision_tree), the criterion is modeled as a &lt;high&gt;sequence of logical TRUE or FALSE questions&lt;/high&gt;.
&lt;br&gt;&lt;br&gt;

]

.pull-right45[

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/tree.png"&gt;
&lt;/p&gt;
]

---

# Classificiation trees

.pull-left45[

Classification trees (and regression trees) are created using a relatively simple &lt;high&gt;three-step algorithm&lt;/high&gt;. 

&lt;u&gt;Algorithm&lt;/u&gt;

1 - &lt;high&gt;Split&lt;/high&gt; nodes to maximize &lt;b&gt;purity gain&lt;/b&gt; (e.g., Gini gain).

2 - &lt;high&gt;Repeat&lt;/high&gt; until pre-defined threshold (e.g., `minsplit`) splits are no longer possible.

3 - &lt;high&gt;Prune&lt;/high&gt; tree to reasonable size.

]

.pull-right45[

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/tree.png"&gt;
&lt;/p&gt;
]

---

# Node splitting

.pull-left45[

Classification trees attempt to &lt;high&gt;minize node impurity&lt;/high&gt; using, e.g., the &lt;high&gt;Gini coefficient&lt;/high&gt;.


`$$\large Gini(S) = 1 - \sum_j^kp_j^2$$`

Nodes are &lt;high&gt;split&lt;/high&gt; using the variable and split value that &lt;high&gt;maximizes Gini gain&lt;/high&gt;. 


`$$Gini \; gain = Gini(S) - Gini(A,S)$$`

with

`$$Gini(A, S) = \sum \frac{n_i}{n}Gini(S_i)$$`

]


.pull-right45[


&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/splitting.png"&gt;
&lt;/p&gt;

]

---

# Pruning trees

.pull-left45[

Classification trees are &lt;high&gt;pruned&lt;/high&gt; back such that every split has a purity gain of at least &lt;high&gt;&lt;mono&gt;cp&lt;/mono&gt;&lt;/high&gt;, with `cp` typically set to `.01`.  

Minimize:

&lt;br&gt;

$$
\large
`\begin{split}
Loss = &amp; Impurity\,+\\
&amp;cp*(n\:terminal\:nodes)\\
\end{split}`
$$

]

.pull-right45[


&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/splitting.png"&gt;
&lt;/p&gt;

]

---

# Pruning trees

.pull-left45[

Classification trees are &lt;high&gt;pruned&lt;/high&gt; back such that every split has a purity gain of at least &lt;high&gt;&lt;mono&gt;cp&lt;/mono&gt;&lt;/high&gt;, with `cp` typically set to `.01`.  

Minimize:

&lt;br&gt;

$$
\large
`\begin{split}
Loss = &amp; Impurity\,+\\
&amp;cp*(n\:terminal\:nodes)\\
\end{split}`
$$

]

.pull-right45[


&lt;p align = "center"&gt;
  &lt;img src="image/cp.png"&gt;
&lt;/p&gt;


]


---

# Regression trees

.pull-left45[

Trees can also be used to perform regression tasks. Instead of impurity, regression trees attempt to &lt;high&gt;minimize within-node variance&lt;/high&gt; (or maximize node homogeneity): 

`$$\large SSE = \sum_{i \in S_1}(y_i - \bar{y}_1)^2+\sum_{i \in S_2}(y_i - \bar{y}_2)^2$$`
&lt;u&gt;Algorithm&lt;/u&gt;

1 - &lt;high&gt;Split&lt;/high&gt; nodes to maximize &lt;b&gt;homogeneity gain&lt;/b&gt;.

2 - &lt;high&gt;Repeat&lt;/high&gt; until pre-defined threshold (e.g., `minsplit`) splits are no longe possible.

3 - &lt;high&gt;Prune&lt;/high&gt; tree to reasonable size.

]



.pull-right45[

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/splitting_regr.png"&gt;
&lt;/p&gt;


]


---
class: center, middle

&lt;font color = "gray"&gt;&lt;h1&gt;Regression&lt;/h1&gt;&lt;/font&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Decision Trees&lt;/h1&gt;&lt;/font&gt;

&lt;high&gt;&lt;h1&gt;Random Forests&lt;/h1&gt;&lt;/high&gt;

---

.pull-left45[

# Random Forest

&lt;p style="padding-top:1px"&gt;&lt;/p&gt;

In [Random Forest](https://en.wikipedia.org/wiki/Random_forest), the criterion is modeled as the &lt;high&gt;aggregate prediction of a large number of decision trees&lt;/high&gt; each based on different features.
&lt;br&gt;

&lt;u&gt;Algorithm&lt;/u&gt;

1 - &lt;high&gt;Repeat&lt;/high&gt; *n* times

&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1 - &lt;high&gt;Resample&lt;/high&gt; data 

&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2 - &lt;high&gt;Grow&lt;/high&gt; non-pruned decision tree

&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Each split &lt;high&gt;consider only &lt;i&gt;m&lt;/i&gt; features&lt;/high&gt;

2 - &lt;high&gt;Average&lt;/high&gt; fitted values

]

.pull-right45[
&lt;br&gt;

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/rf.png"&gt;
&lt;/p&gt;


]

---

# Random Forest

.pull-left45[

&lt;p style="padding-top:1px"&gt;&lt;/p&gt;

Random forests make use of important machine learning elements, &lt;high&gt;resampling&lt;/high&gt; and &lt;high&gt;averaging&lt;/high&gt; that together are also referred to as &lt;high&gt;bagging&lt;/high&gt;. 


&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
  &lt;col width="30%"&gt;
  &lt;col width="70%"&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Element&lt;/b&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Description&lt;/b&gt;
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Resampling&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    Creates new data sets that vary in their composition thereby &lt;high&gt;deemphasizing idiosyncracies&lt;/high&gt; of the available data. 
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Averaging&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    Combining predictions typically &lt;high&gt;evens out idiosyncracies&lt;/high&gt; of the models created from single data sets.   
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;/table&gt;
]


.pull-right45[

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/tree_crowd.png"&gt;
&lt;/p&gt;


]


---

# Random Forest

.pull-left45[

&lt;p style="padding-top:1px"&gt;&lt;/p&gt;

Random forests make use of important machine learning elements, &lt;high&gt;resampling&lt;/high&gt; and &lt;high&gt;averaging&lt;/high&gt; that together are also referred to as &lt;high&gt;bagging&lt;/high&gt;. 


&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
  &lt;col width="30%"&gt;
  &lt;col width="70%"&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Element&lt;/b&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Description&lt;/b&gt;
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Resampling&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    Creates new data sets that vary in their composition thereby &lt;high&gt;deemphasizing idiosyncracies&lt;/high&gt; of the available data. 
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Averaging&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    Combining predictions typically &lt;high&gt;evens out idiosyncracies&lt;/high&gt; of the models created from single data sets.   
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;/table&gt;
]


.pull-right45[

&lt;p align = "center"&gt;
  &lt;img src="image/mtry_parameter.png"&gt;
&lt;/p&gt;


]


---

class: center, middle

&lt;font color = "gray"&gt;&lt;h1&gt;Fitting&lt;/h1&gt;&lt;/font&gt;

&lt;font color = "gray"&gt;&lt;h1&gt;Evaluation&lt;/h1&gt;&lt;/font&gt;

&lt;a&gt;&lt;h1&gt;Tuning&lt;/h1&gt;&lt;/a&gt;


---

# Tuning

.pull-left45[

All machine learning models are equipped with tuning parameters that &lt;high&gt; control model complexity&lt;high&gt;. 

These tuning parameters can be identified using a &lt;high&gt;validation set&lt;/high&gt; created from the traning data.

&lt;u&gt;Logic&lt;/u&gt;

1 - Create separate test set. 

2 - Fit model using various tuning parameters.

3 - Select tuning leading to best prediction on
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;validation set.

4 - Refit model to entire training set (training + 
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;validation).


]

.pull-right45[

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/validation.png" height=430px&gt;
&lt;/p&gt;

]

---

# Resampling methods

.pull-left4[

Resampling methods automatize and generalize model tuning. 

&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
  &lt;col width="30%"&gt;
  &lt;col width="70%"&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Method&lt;/b&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Description&lt;/b&gt;
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;k-fold cross-validation&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    Splits the data in k-pieces, use &lt;high&gt;each piece once&lt;/high&gt; as the validation set, while using the other one for training. 
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Bootstrap&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    For &lt;i&gt;B&lt;/i&gt; bootstrap rounds &lt;high&gt;sample&lt;/high&gt; from the data &lt;high&gt;with replacement&lt;/high&gt; and split the data in training and validation set.  
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;/table&gt;
]

.pull-right5[

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/resample1.png"&gt;
&lt;/p&gt;

]

---

# Resampling methods

.pull-left4[

Resampling methods automatize and generalize model tuning. 

&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
  &lt;col width="30%"&gt;
  &lt;col width="70%"&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Method&lt;/b&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Description&lt;/b&gt;
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;k-fold cross-validation&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    Splits the data in k-pieces, use &lt;high&gt;each piece once&lt;/high&gt; as the validation set, while using the other one for training. 
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Bootstrap&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    For &lt;i&gt;B&lt;/i&gt; bootstrap rounds &lt;high&gt;sample&lt;/high&gt; from the data &lt;high&gt;with replacement&lt;/high&gt; and split the data in training and validation set.  
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;/table&gt;
]

.pull-right5[

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/resample2.png"&gt;
&lt;/p&gt;

]

---

# Resampling methods

.pull-left4[

Resampling methods automatize and generalize model tuning. 

&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
  &lt;col width="30%"&gt;
  &lt;col width="70%"&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Method&lt;/b&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    &lt;b&gt;Description&lt;/b&gt;
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;k-fold cross-validation&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    Splits the data in k-pieces, use &lt;high&gt;each piece once&lt;/high&gt; as the validation set, while using the other one for training. 
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td bgcolor="white"&gt;
    &lt;i&gt;Bootstrap&lt;/i&gt;
  &lt;/td&gt;
  &lt;td bgcolor="white"&gt;
    For &lt;i&gt;B&lt;/i&gt; bootstrap rounds &lt;high&gt;sample&lt;/high&gt; from the data &lt;high&gt;with replacement&lt;/high&gt; and split the data in training and validation set.  
  &lt;/td&gt;  
&lt;/tr&gt;
&lt;/table&gt;
]

.pull-right5[

&lt;p align = "center" style="padding-top:0px"&gt;
&lt;img src="image/resample3.png"&gt;
&lt;/p&gt;

]

---

.pull-left45[

# &lt;i&gt;k&lt;/i&gt;-fold cross validation for Ridge and Lasso

&lt;p style="padding-top:1px"&gt;&lt;/p&gt;

&lt;b&gt;Goal&lt;/b&gt;

Use 10-fold cross-validation to identify &lt;high&gt;optimal regularization parameters&lt;/high&gt; for a regression model. 

&lt;b&gt;Consider&lt;/b&gt;

`\(\alpha \in 0, .5, 1\)` and `\(\lambda \in 1, 2, ..., 100\)`

]


.pull-right45[

&lt;br&gt;&lt;br&gt;&lt;br&gt;

&lt;p align = "center"&gt;
  &lt;img src="image/lasso_process.png" height=460px&gt;
&lt;/p&gt;


]


---

class: middle, center

&lt;h1&gt;&lt;a href=https://cdsbasel.github.io/dataanalytics/menu/materials.html&gt;Material&lt;/a&gt;&lt;/h1&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
